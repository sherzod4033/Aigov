Ниже — разбор текущей схемы (что реально ухудшает качество/скорость/полноту) и затем вариант “лучшего” чанкинга для неструктурированных документов, который можно внедрить у вас без зоопарка веток и с нормальной поддержкой mixed PDF, overlap, заголовков/списков/таблиц, и без LLM на каждом шаге.
1) Что в текущем чанкинге объективно мешает качеству
1) Потеря контента в mixed PDF (самое критичное)
Сейчас is_scanned_pdf решает “скан/не скан” на уровне всего PDF: если хоть одна страница имеет текстовый слой — OCR не запускается вообще.
Итог: в смешанных PDF (часть страниц — изображения/сканы) страницы без текста просто пропадают из индекса.
Правило №1 для ingestion PDF: OCR должен быть постраничным, а не “весь документ OCR / весь документ без OCR”.
2) Чанкинг “по страницам” ломает семантику и снижает recall
Для обычного PDF вы делаете: PyMuPDF -> page_text -> agentic_chunk_text(page).
Проблемы:
разрывы по границе страницы часто не совпадают с логическими границами;
заголовок на одной странице, тело на следующей — попадёт в разные чанки без связки;
retrieval начинает вытаскивать “половинки” смысла;
ваш check_semantic_boundary становится почти бесполезным, потому что контекст часто слишком короткий.
3) CHUNK_OVERLAP есть, но в основной ветке не работает
Overlap есть только в semantic-ветке OCR, а основной путь (agentic) идёт без overlap.
Это почти гарантированно ухудшает:
ответы на вопросы, которые “цепляют” конец/начало абзаца,
извлечение определений/условий, которые часто живут на стыках.
4) LLM boundary-check почти никогда не включается
Вы сами это заметили: ранний выход < 500 символов в check_semantic_boundary, плюс у вас чанкинг идёт мелкими порциями (страница, предложение), поэтому LLM “YES/NO” либо не вызывается, либо вызывается не там, где он реально полезен.
5) RAGService создаётся слишком часто
agentic_chunk_text создаёт RAGService внутри себя; для PDF это фактически на каждую страницу.
Это:
лишняя инициализация,
лишние warmup/подключения,
лишняя задержка и нестабильность под нагрузкой.
6) Размеры в символах вместо токенов
Символы — плохая метрика:
русский/таджикский/армянский/юридический текст токенизируется иначе, чем английский;
embedding/LLM-пайплайн живёт в токенах;
“2000 символов” иногда это 250 токенов, а иногда 900+.
2) Каким должен быть “лучший” чанкинг для неструктурированных документов
Цели (практичные):
Полнота: ничего не теряем (особенно mixed PDF).
Структурность: не ломаем заголовки, списки, таблицы, “Статья 12”, “Глава 3”, нумерации.
Стабильность: детерминированные границы, повторяемые результаты (важно для обновлений/переиндексации).
Релевантность: чанк достаточно “узкий” для точного поиска, но достаточно “широкий” для ответа.
Экономичность: минимизируем LLM-вызовы; используем эвристики + embeddings (если нужно) батчами.
Единый пайплайн: один чанкер для PDF/DOCX/TXT/OCR, различается только extraction.
3) Предлагаемая архитектура: один “Hybrid Layout Chunker”
Вместо “agentic vs semantic ветка” сделайте один чанкер, который работает с универсальным промежуточным форматом:
3.1. Универсальный формат: Blocks → Units → Chunks
Block — минимальный фрагмент, полученный на extraction-этапе:
text
page (или page_start/page_end)
(опционально) bbox (для PDF/OCR)
order (порядок чтения)
(опционально) source = "pymupdf" / "ocr" / "docx" / "txt"
Unit — структурная единица (после нормализации и классификации):
kind: heading | paragraph | list_item | table_like | code_like
text
page_range
section_path (стек заголовков: ["ГЛАВА 2", "Статья 5"])
Chunk — то, что уходит в БД/Chroma:
text (или text_clean + text_for_embedding)
page_range
section_path
chunk_index
char_span (если нужно подсвечивать в UI)
стабильный chunk_id
4) Extraction: как исправить PDF (включая mixed)
4.1. Переход на постраничную диагностику “нужен OCR / не нужен”
Вместо is_scanned_pdf(document):

page_needs_ocr(page_text):
извлекли page_text через PyMuPDF;
если len(page_text.strip()) < threshold_chars (например 40–120 символов) → OCR;
можно усилить правилом: доля букв/цифр среди символов < X;
для совсем “мусорного” текста (битый слой) — тоже OCR.
Результат: по каждой странице выбирается источник текста: текстовый слой или OCR.
4.2. Не “page -> chunk”, а “page -> blocks -> общий поток”
Из PDF лучше доставать блоки (а не “вся страница строкой”), чтобы:
не резать таблицы/списки как попало,
лучше собирать абзацы,
проще удалять headers/footers.
В PyMuPDF это обычно делается через get_text("dict")/get_text("blocks") с сохранением порядка.
4.3. Удаление повторяющихся headers/footers (очень полезно для OCR и PDF)
Простая эвристика:
взять первые 1–2 строки и последние 1–2 строки каждой страницы,
нормализовать (trim, множественные пробелы),
посчитать частоты,
строки, которые повторяются на >60% страниц и короткие (<80–100 символов) — выкинуть.
Это резко снижает “шума” в Chroma.
5) Нормализация и структурирование: делаем одинаково для всех типов
5.1. Нормализация текста (общая)
привести пробелы/переносы: \r\n → \n, множественные пробелы → один;
склеить переносы внутри абзаца, но сохранить пустые строки как разделители;
осторожная де-дефисация: (\w)-\n(\w) → \1\2 (но не для “по‑русски” с дефисами — это надо ограничить условием “перенос строки сразу после дефиса и без пробелов”).
5.2. Классификация строк/абзацев в Unit.kind
Нужны 3–4 детектора (эвристики, дешево и эффективно):
Heading detector (работает и для вашего OCR “СТАТЬЯ/ГЛАВА”, и для обычных PDF/DOCX):
совпадение с шаблонами:
^(СТАТЬЯ|ГЛАВА|РАЗДЕЛ|ГЛАВА|БОБИ|МОДДАИ)\s+\d+
^\d+(\.\d+)*\s+\S+ (многоуровневая нумерация)
^[IVXLCDM]+\.\s+\S+ (римские)
или “короткая строка, много заглавных, нет точки в конце”.
List detector
^-  / ^• / ^\d+\) / ^\d+\. / ^[a-zа-я]\) и т.п.
если подряд много строк такого вида — это список.
Table-like detector (минимально)
строки с большим числом разделителей | или множественными колонками пробелами,
одинаковое число “колонок” в 3+ строках подряд.
Code-like detector (если встречается)
высокая доля символов {}();<> или моноширинные блоки (в DOCX можно понять по стилю, но можно и эвристикой).
6) Сам чанкер: “packing” по токенам + правильный overlap
6.1. Почему packing должен быть по токенам
Векторный поиск и LLM работают в токенах. Для русского/юридического текста символы дают огромный разброс.
Практическая настройка (стартовая):
target_tokens: 350–500
max_tokens: 650–900
min_tokens: 150–250
overlap_tokens: 40–120 (обычно 10–20% от target)
Если у вас retrieval топ-K маленький (например 3–5), overlap можно сделать чуть больше.
Если вы не хотите тащить токенайзер — начните с аппроксимации tokens ≈ len(text) / 3.6 для кириллицы (это грубо, но лучше символов 1:1).
6.2. Packing-алгоритм (детерминированный, без LLM)
Идём по Unit последовательно:
Начинаем новый chunk.
Добавляем units пока не приблизились к target_tokens.
Если следующий unit переполнит max_tokens:
если текущий chunk уже >= min_tokens → закрываем chunk,
иначе пытаемся разрезать длинный unit (по абзацам → предложениям → hard split).
Границы стараемся ставить на:
границе раздела/статьи/главы,
границе абзаца,
границе списка (не разрывать список посередине, если можно),
границе предложения.
Это уже даёт качество выше вашего текущего agentic, потому что:
не “страница отдельно”,
заголовки учитываются везде,
списки/таблицы не рвутся случайно,
нет случайных LLM “YES/NO”.
6.3. Overlap: как сделать правильно и одинаково для всех
Вариант “просто приписать tail предыдущего чанка к следующему” рабочий, но раздувает индекс.
Лучший вариант (без раздувания Chroma):
храните чанки без overlap,
но в retrieval-этапе делайте neighbor expansion:
если нашли chunk i, добавьте i-1 и i+1 (по chunk_index) в контекст LLM (или хотя бы один сосед, если лимит).

Плюсы:
не дублируете текст в embeddings,
индекс меньше,
качество ответа часто выше, чем от overlap.
Если сейчас проще внедрить overlap в ingestion — ок, но стратегически соседей добавить лучше.
7) “Семантический” контроль границ без LLM: через embeddings
Ваш текущий check_semantic_boundary через LLM:
дорогой,
нестабильный,
и почти не работает из-за порога 500 символов.
Замена, которая реально работает в RAG:
На этапе chunking (или даже до него) посчитайте embedding для каждого абзаца/юнита батчем.
Посчитайте similarity соседних юнитов.
Там, где similarity резко падает (ниже порога), — это хороший кандидат на границу темы.
Это essentially “TextTiling”-подход, но на современных embeddings.
Как применять без усложнений:
делайте “soft boundaries”:
heading → всегда boundary,
similarity < T → boundary-кандидат,
дальше packing старается закрывать chunk рядом с boundary-кандидатами.
Так вы получаете “семантику” без LLM и без ветки “semantic vs agentic”.
8) Конкретный план изменений в вашем проекте (минимально болезненно)
Шаг 1. Убрать разветвление на два разных чанкинга
В backend/app/api/endpoints/documents.py и document_service.py:
оставить разветвление только на extraction (PDF text layer / OCR / DOCX / TXT),
но результат всегда приводить к blocks[],
дальше один chunker.chunk(blocks).
Шаг 2. Сделать OCR постраничным
Заменить is_scanned_pdf на:
цикл по страницам:
text = extract_text_layer(page)
если page_needs_ocr(text) → text = ocr(page_image)
добавить блоки в общий список
Шаг 3. Перестать чанкать “по странице”
Собирайте единый поток blocks с page метаданными.
Граница страницы — это просто метка page, но не причина резать chunk.
Шаг 4. Overlap либо в ingestion, либо “neighbor expansion” в retrieval
Быстро: добавить overlap в общий chunker.
Лучше: сделать соседей в retrieval и хранить чанки без overlap.
Шаг 5. Убрать RAGService из agentic_chunk_text
Если остаётся что-то “семантическое” — пусть это будет embedding similarity, и сервис создаётся один раз на документ (или singleton через DI).
Иначе — вообще не нужен на этапе чанкинга.
Шаг 6. Батчить запись в БД и Chroma
Сейчас “каждый chunk → сразу БД + индекс”. Лучше:
сначала сформировать все чанки,
одним батчем записать в БД,
одним батчем посчитать embeddings,
одним батчем upsert в Chroma.
Это и быстрее, и проще откатывать при ошибках.
9) Эталонный псевдокод (как это должно выглядеть)
Копировать код
Python
@dataclass
class TextBlock:
    text: str
    page: int
    order: int
    bbox: tuple[float, float, float, float] | None = None
    source: str = "unknown"

@dataclass
class Unit:
    text: str
    kind: str  # heading|paragraph|list_item|table_like|code_like
    page_start: int
    page_end: int
    order: int
    section_path: list[str]

@dataclass
class Chunk:
    chunk_index: int
    text: str
    page_start: int
    page_end: int
    section_path: list[str]

class HybridChunker:
    def init(self, target_tokens=450, max_tokens=800, min_tokens=200, overlap_tokens=80):
        ...

    def chunk(self, blocks: list[TextBlock]) -> list[Chunk]:
        blocks = self.normalize_blocks(blocks)
        units = self.blocks_to_units(blocks)           # headings/lists/tables detection
        chunks = self.pack_units(units)                # token-based packing
        chunks = self.postprocess(chunks)              # cleanup, min-size merges
        # либо overlap здесь, либо neighbor expansion потом
        # chunks = self.apply_overlap(chunks)
        return chunks
10) “Быстрые победы” (если надо улучшить сильно, но быстро)
Если вы не готовы к крупной переработке сразу, вот порядок максимального эффекта:
OCR постранично для mixed PDF (иначе вы теряете данные).
Убрать чанкинг строго по страницам: хотя бы объединяйте страницы в поток до чанкинга.
Включить overlap в основной ветке (или лучше — retrieval neighbor expansion).

RAGService создавать один раз, а не внутри agentic_chunk_text.
Перейти с символов на токены (или их аппроксимацию).
11) Рекомендованный “лучший” итоговый вариант (коротко)
Один unified pipeline:
Extract → Blocks (per-page OCR when needed) → Normalize (remove headers/footers) → Units (headings/lists/tables) → Token-based packing → (optional embedding-based boundary hints) → Store chunks → Index (batch)
И для качества retrieval я бы выбрал:
чанки без overlap + neighbor expansion при сборке контекста.
Это даст лучший баланс:
качество ответов на стыках,
меньше дублирования в Chroma,
проще сопровождение.
Если хочешь, я могу написать конкретный “drop-in” дизайн под ваши текущие функции/классы (какие функции заменить, какие сигнатуры сделать, где хранить chunk_index и как сделать neighbor expansion при retrieval) — но даже без этого, описанная схема выше уже даёт понятный “правильный север” и конкретные шаги миграции.